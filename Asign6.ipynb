{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 6390: Assignment 6\n",
    "## Due November 1st\n",
    "### By: Nicholas Lutrzykowski \n",
    "The goal of this assignment is to use Multi Class Logistic Regression on the Fashion MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements \n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and setup data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data_orig = np.genfromtxt(filename, delimiter=',', dtype=\"i8\")\n",
    "\n",
    "    # First column is the ground truths \n",
    "    # First row is labels which we want excluded\n",
    "    data = data_orig[1:, :]\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    num_points = data.shape[0]\n",
    "    num_attributes = data.shape[1]-1\n",
    "    print(filename)\n",
    "    print(\"Number of attributes:\", num_attributes)\n",
    "    print(\"Number of points:\", num_points)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion-mnist_test.csv\n",
      "Number of attributes: 784\n",
      "Number of points: 10000\n",
      "fashion-mnist_train.csv\n",
      "Number of attributes: 784\n",
      "Number of points: 60000\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST = 5000 \n",
    "NUM_TRAINING = 10000\n",
    "\n",
    "test = read_data('fashion-mnist_test.csv')[:NUM_TEST, :]\n",
    "training = read_data('fashion-mnist_train.csv')[:NUM_TRAINING, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Logistic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_order(D, rand):\n",
    "    if rand:\n",
    "        np.random.shuffle(D)\n",
    "    y_temp = D[:, 0].astype(int)\n",
    "    y = np.zeros((y_temp.shape[0], np.amax(y_temp)+1))\n",
    "    y[np.arange(y_temp.shape[0]), y_temp] = 1\n",
    "    \n",
    "    D = np.concatenate((np.ones((D.shape[0], 1)), D[:, 1:]), axis=1)\n",
    "    \n",
    "    return D, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_multi_class(D_orig, eta, epsilon):\n",
    "    # Initialize y, D, W\n",
    "    D, y = get_rand_order(D_orig, False)\n",
    "    \n",
    "    W = np.zeros((D.shape[1], y.shape[1]))\n",
    "    W_prev = W \n",
    "    dif = 2*epsilon \n",
    "    \n",
    "    while dif > epsilon: \n",
    "        D, y = get_rand_order(D_orig, True)\n",
    "        \n",
    "        # axis = 1 (we want to compute softmax about the columns)\n",
    "        pi = softmax(np.matmul(D, W), axis=1) \n",
    "        gradient = np.matmul((y-pi).T, D).T\n",
    "        W = W_prev + eta*gradient\n",
    "        \n",
    "        dif = np.sum((W-W_prev)**2)\n",
    "        W_prev = W\n",
    "    \n",
    "    return W\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights after training are:\n",
      "[[ 4.133e-04 -5.376e-04 -1.269e-03 ... -1.770e-03 -6.738e-03 -1.301e-02]\n",
      " [-1.936e-04 -4.600e-06 -2.560e-05 ... -4.600e-06 -6.106e-04 -1.060e-05]\n",
      " [-1.032e-03 -1.087e-04  7.622e-04 ... -1.469e-05 -7.847e-05 -7.173e-05]\n",
      " ...\n",
      " [-1.172e-01 -1.672e-02  2.353e-01 ...  7.341e-04 -1.223e-01 -6.656e-02]\n",
      " [-4.983e-02  1.832e-03  6.784e-02 ...  3.665e-03 -7.626e-02 -3.627e-02]\n",
      " [-7.379e-03 -2.141e-04  3.071e-03 ... -7.551e-04 -1.048e-02 -3.651e-03]]\n"
     ]
    }
   ],
   "source": [
    "W = logistic_regression_multi_class(training, 1e-6, 2.5)\n",
    "\n",
    "print(\"The weights after training are:\")\n",
    "np.set_printoptions(precision=3)\n",
    "print(W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_measure(y, y_hat, num_classes):\n",
    "    # Find number of correct predictions of class ci / the number of points in class ci \n",
    "    Fi = [] \n",
    "    for i in range(num_classes):\n",
    "        ni = np.where(y==i)[0].shape[0]\n",
    "        res = np.where((y-y_hat) == 0, y, -1)\n",
    "        nii = np.sum(np.where(res == i, 1, 0))\n",
    "        mi = np.where(y_hat==i)[0].shape[0]\n",
    "\n",
    "        Fi.append((2*nii)/(ni+mi))\n",
    "    \n",
    "    F = np.sum(np.array(Fi))/num_classes\n",
    "        \n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(data, y, w): \n",
    "    w = np.reshape(w, (w.shape[0], 1))\n",
    "    y_hat = np.matmul(data, w)\n",
    "    y_hat = np.where(y_hat < 0, -1, 1)\n",
    "    y_hat = np.reshape(y_hat, (data.shape[0],))\n",
    "    accuracy = 1 - (data.shape[0] - np.count_nonzero(y_hat-y))/data.shape[0]\n",
    "    \n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 82.92%\n",
      "The F1-Score is: 0.8306\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy on test data\n",
    "D, y = get_rand_order(test, False)\n",
    "\n",
    "y_hat = np.argmax(softmax(np.matmul(D, W), axis=1), axis=1)\n",
    "y = np.argmax(y, axis=1)\n",
    "accuracy = 1- np.sum(np.where((y-y_hat) != 0, 1, 0))/NUM_TEST\n",
    "\n",
    "f_measure = get_f_measure(y, y_hat, 10)\n",
    "\n",
    "print(\"The accuracy is: {0:.2f}%\".format(accuracy*100))\n",
    "print(\"The F1-Score is: {0:.4f}\".format(f_measure))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results match the expected performance of the Multi-Class Logistic Algorithm. After adjusting the epsilon value we were able to slightly improve results while still having a reasonable training run-time of under a minute. If I decreased the epsilon value much more, then the weights would never converge, resulting in an infinite loop.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99ef57a3e309b3ccef27bfdc21155d00f14e44d33227cb907457916f15963b11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
